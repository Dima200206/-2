{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dima200206/-2/blob/main/%D0%9B%D0%913_%D0%9A%D0%A2%D0%9E%D0%92%D0%94%20_%D0%A4%D0%86%D0%A2_1-4%D0%BC_%D0%92%D0%BB%D0%B0%D1%81%D0%B5%D0%BD%D0%BA%D0%BE_%D0%94%D0%BC%D0%B8%D1%82%D1%80%D0%BE_GIT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞ —Ä–æ–±–æ—Ç–∞: –†–æ–±–æ—Ç–∞ –∑ RDD —É PySpark\n",
        "\n",
        "# ============================================================\n",
        "# –ß–ê–°–¢–ò–ù–ê 1. –°–¢–í–û–†–ï–ù–ù–Ø RDD –¢–ê –ë–ê–ó–û–í–Ü –û–ü–ï–†–ê–¶–Ü–á\n",
        "# ============================================================\n",
        "\n",
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "import random\n",
        "\n",
        "# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ PySpark\n",
        "spark = SparkSession.builder.appName(\"Lab3_RDD\").master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "print(\"‚úÖ SparkContext —Å—Ç–≤–æ—Ä–µ–Ω–æ:\", sc)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è RDD –∑—ñ —Å–ø–∏—Å–∫—É —Å–ª—ñ–≤\n",
        "# ------------------------------------------------------------\n",
        "words = [\"apple\", \"banana\", \"orange\", \"blueberry\", \"blackberry\", \"grape\", \"banana\", \"apple\", \"berry\"] * 12\n",
        "rdd_words = sc.parallelize(words)\n",
        "print(\"–ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ —É RDD:\", rdd_words.count())\n",
        "print(\"–ü–µ—Ä—à—ñ 10 –µ–ª–µ–º–µ–Ω—Ç—ñ–≤:\", rdd_words.take(10))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó –Ω–∞–¥ RDD\n",
        "# ------------------------------------------------------------\n",
        "rdd_upper = rdd_words.map(lambda x: x.upper())\n",
        "rdd_b = rdd_upper.filter(lambda x: x.startswith(\"B\"))\n",
        "rdd_distinct = rdd_b.distinct()\n",
        "\n",
        "print(\"\\nüîπ –°–ª–æ–≤–∞, —â–æ –ø–æ—á–∏–Ω–∞—é—Ç—å—Å—è –∑ B:\", rdd_b.collect())\n",
        "print(\"üîπ –£–Ω—ñ–∫–∞–ª—å–Ω—ñ —Å–ª–æ–≤–∞:\", rdd_distinct.collect())\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3. –†–æ–±–æ—Ç–∞ –∑ —á–∏—Å–ª–æ–≤–∏–º–∏ –¥–∞–Ω–∏–º–∏\n",
        "# ------------------------------------------------------------\n",
        "numbers = [random.randint(1, 100) for _ in range(100)]\n",
        "rdd_numbers = sc.parallelize(numbers)\n",
        "\n",
        "rdd_squared = rdd_numbers.map(lambda x: x ** 2)\n",
        "rdd_even = rdd_squared.filter(lambda x: x % 2 == 0)\n",
        "even_count = rdd_even.count()\n",
        "\n",
        "print(\"\\nüîπ –ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–∞—Ä–Ω–∏—Ö —á–∏—Å–µ–ª:\", even_count)\n",
        "print(\"üîπ –ü–µ—Ä—à—ñ 10 –∫–≤–∞–¥—Ä–∞—Ç—ñ–≤:\", rdd_squared.take(10))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4. –ê–≥—Ä–µ–≥–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö\n",
        "# ------------------------------------------------------------\n",
        "sum_all = rdd_numbers.reduce(lambda a, b: a + b)\n",
        "product_all = rdd_numbers.reduce(lambda a, b: a * b)\n",
        "\n",
        "print(\"\\nüîπ –°—É–º–∞ –≤—Å—ñ—Ö —á–∏—Å–µ–ª:\", sum_all)\n",
        "print(\"üîπ –î–æ–±—É—Ç–æ–∫ —É—Å—ñ—Ö —á–∏—Å–µ–ª:\", product_all)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5. –ö–µ—à—É–≤–∞–Ω–Ω—è —Ç–∞ –ø–æ–≤—Ç–æ—Ä–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è RDD\n",
        "# ------------------------------------------------------------\n",
        "rdd_numbers.cache()\n",
        "print(\"\\n–ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ (1-–π —Ä–∞–∑):\", rdd_numbers.count())\n",
        "print(\"–ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ (2-–π —Ä–∞–∑ —ñ–∑ –∫–µ—à—É):\", rdd_numbers.count())\n",
        "\n",
        "# ============================================================\n",
        "# –ß–ê–°–¢–ò–ù–ê 2. –ü–†–ê–ö–¢–ò–ß–ù–Ü –ó–ê–í–î–ê–ù–ù–Ø\n",
        "# ============================================================\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1. –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —Å–ª—ñ–≤ —É –¥–∞—Ç–∞—Å–µ—Ç—ñ\n",
        "# ------------------------------------------------------------\n",
        "sample_text = [\"This is an example of PySpark word count.\",\n",
        "               \"PySpark RDD operations are very powerful.\",\n",
        "               \"This example demonstrates map and reduceByKey in Spark.\"]\n",
        "\n",
        "text_rdd = sc.parallelize(sample_text)\n",
        "words = text_rdd.flatMap(lambda line: line.split(\" \"))\n",
        "word_counts = words.map(lambda x: (x.lower(), 1)).reduceByKey(lambda a, b: a + b)\n",
        "print(\"\\nüîπ –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ª—ñ–≤ —É –¥–∞—Ç–∞—Å–µ—Ç—ñ:\")\n",
        "for w, c in word_counts.collect():\n",
        "    print(f\"{w}: {c}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2. –ê–Ω–∞–ª—ñ–∑ –ª–æ–≥—ñ–≤ —Å–µ—Ä–≤–µ—Ä–∞\n",
        "# ------------------------------------------------------------\n",
        "logs_data = [\n",
        "    \"127.0.0.1 - - [10/Oct/2023:13:55:36] 'GET /index.html HTTP/1.1' 200 2326\",\n",
        "    \"192.168.1.1 - - [10/Oct/2023:14:55:36] 'POST /login HTTP/1.1' 404 452\",\n",
        "] * 60  # 120 –∑–∞–ø–∏—Å—ñ–≤\n",
        "\n",
        "logs = sc.parallelize(logs_data)\n",
        "\n",
        "# –£—Å–ø—ñ—à–Ω—ñ –∑–∞–ø–∏—Ç–∏ (200)\n",
        "success = logs.filter(lambda line: \" 200 \" in line)\n",
        "ip_counts = success.map(lambda line: (line.split(\" \")[0], 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "print(\"\\nüîπ –¢–æ–ø 10 IP —ñ–∑ —É—Å–ø—ñ—à–Ω–∏–º–∏ –∑–∞–ø–∏—Ç–∞–º–∏:\")\n",
        "print(ip_counts.take(10))\n",
        "\n",
        "# –ì—Ä—É–ø—É–≤–∞–Ω–Ω—è –∑–∞ —Å—Ç–∞—Ç—É—Å–æ–º\n",
        "grouped = logs.map(lambda line: (line.split(\" \")[-2], line)).groupByKey()\n",
        "print(\"\\nüîπ –ì—Ä—É–ø—É–≤–∞–Ω–Ω—è –∑–∞ —Å—Ç–∞—Ç—É—Å–∞–º–∏:\")\n",
        "for status, group in grouped.collect():\n",
        "    print(status, \"‚Üí\", len(list(group)))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3. –ê–Ω–∞–ª—ñ–∑ —Ç–µ–∫—Å—Ç—É (—ñ–º—ñ—Ç–∞—Ü—ñ—è –≤–µ–ª–∏–∫–æ—ó —Å—Ç–∞—Ç—Ç—ñ)\n",
        "# ------------------------------------------------------------\n",
        "article_text = \"\"\"\n",
        "PySpark is an interface for Apache Spark in Python. It allows you to write Spark applications using Python APIs.\n",
        "PySpark supports most of Spark‚Äôs features such as Spark SQL, DataFrame, Streaming, MLlib (Machine Learning) and Spark Core.\n",
        "Spark is designed to cover a wide range of workloads such as batch applications, iterative algorithms, interactive queries and streaming.\n",
        "\"\"\" * 300  # ~1MB —Ç–µ–∫—Å—Ç—É\n",
        "\n",
        "article = sc.parallelize(article_text.split(\"\\n\"))\n",
        "\n",
        "words = (article\n",
        "         .flatMap(lambda line: line.lower().split())\n",
        "         .filter(lambda w: w.isalpha() and len(w) > 3))\n",
        "\n",
        "word_freq = words.map(lambda w: (w, 1)).reduceByKey(lambda a, b: a + b)\n",
        "top_words = word_freq.takeOrdered(20, key=lambda x: -x[1])\n",
        "print(\"\\nüîπ 20 –Ω–∞–π—á–∞—Å—Ç—ñ—à–∏—Ö —Å–ª—ñ–≤ —É —Å—Ç–∞—Ç—Ç—ñ:\")\n",
        "for w, c in top_words:\n",
        "    print(f\"{w}: {c}\")\n",
        "\n",
        "# –ù–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–µ —Å–ª–æ–≤–æ\n",
        "top_word = top_words[0][0]\n",
        "print(\"\\nüîπ –ù–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–µ —Å–ª–æ–≤–æ:\", top_word)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4. Broadcast-–∑–º—ñ–Ω–Ω—ñ\n",
        "# ------------------------------------------------------------\n",
        "transactions = sc.parallelize([101,102,101,103,102,101,104,103,104,102]*10)\n",
        "products = {101: \"–ù–æ—É—Ç–±—É–∫\", 102: \"–°–º–∞—Ä—Ç—Ñ–æ–Ω\", 103: \"–ü–ª–∞–Ω—à–µ—Ç\", 104: \"–ú–æ–Ω—ñ—Ç–æ—Ä\"}\n",
        "\n",
        "broadcast_products = sc.broadcast(products)\n",
        "mapped = transactions.map(lambda x: (broadcast_products.value[x], 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "print(\"\\nüîπ –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –ø—Ä–æ–¥–∞–∂—ñ–≤ –∑–∞ —Ç–æ–≤–∞—Ä–∞–º–∏:\")\n",
        "for k, v in mapped.collect():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5. –ê–∫—É–º—É–ª—è—Ç–æ—Ä–Ω—ñ –∑–º—ñ–Ω–Ω—ñ\n",
        "# ------------------------------------------------------------\n",
        "high_temp_acc = sc.accumulator(0)\n",
        "temps = sc.parallelize([25, 31, 28, 35, 30, 33, 29, 40, 22, 31])\n",
        "\n",
        "def check_temp(t):\n",
        "    global high_temp_acc\n",
        "    if t > 30:\n",
        "        high_temp_acc.add(1)\n",
        "    return t\n",
        "\n",
        "temps.map(check_temp).collect()\n",
        "print(\"\\nüîπ –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∞–Ω–æ–º–∞–ª—å–Ω–æ –≤–∏—Å–æ–∫–∏—Ö —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä:\", high_temp_acc.value)\n",
        "\n",
        "# ============================================================\n",
        "# –í–ò–°–ù–û–í–ö–ò\n",
        "# ============================================================\n",
        "\n",
        "print(\"\"\"\n",
        "‚úÖ –í–ò–°–ù–û–í–ö–ò:\n",
        "- –°—Ç–≤–æ—Ä–µ–Ω–æ —Ç–∞ –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–æ RDD-—Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –≤ PySpark.\n",
        "- –í–∏–∫–æ–Ω–∞–Ω–æ –±–∞–∑–æ–≤—ñ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó (map, filter, distinct, reduceByKey).\n",
        "- –ü—Ä–æ–≤–µ–¥–µ–Ω–æ –∞–≥—Ä–µ–≥–∞—Ü—ñ—é —Ç–∞ –∫–µ—à—É–≤–∞–Ω–Ω—è.\n",
        "- –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ broadcast- —ñ accumulator-–∑–º—ñ–Ω–Ω—ñ.\n",
        "- –ü—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–æ–≤–∞–Ω–æ —Ä–æ–±–æ—Ç—É –∑ —Ç–µ–∫—Å—Ç–∞–º–∏, –ª–æ–≥–∞–º–∏, —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—è–º–∏ —Ç–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞–º–∏.\n",
        "PySpark –¥–∞—î –∑–º–æ–≥—É –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–æ–±–ª—è—Ç–∏ –≤–µ–ª–∏–∫—ñ –æ–±—Å—è–≥–∏ –¥–∞–Ω–∏—Ö —É —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–æ–º—É —Å–µ—Ä–µ–¥–æ–≤–∏—â—ñ.\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "oluoRDN0PCGn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7826f5ed-4392-4ba2-f9c9-22958fc2794f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "‚úÖ SparkContext —Å—Ç–≤–æ—Ä–µ–Ω–æ: <SparkContext master=local[*] appName=Lab3_RDD>\n",
            "–ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ —É RDD: 108\n",
            "–ü–µ—Ä—à—ñ 10 –µ–ª–µ–º–µ–Ω—Ç—ñ–≤: ['apple', 'banana', 'orange', 'blueberry', 'blackberry', 'grape', 'banana', 'apple', 'berry', 'apple']\n",
            "\n",
            "üîπ –°–ª–æ–≤–∞, —â–æ –ø–æ—á–∏–Ω–∞—é—Ç—å—Å—è –∑ B: ['BANANA', 'BLUEBERRY', 'BLACKBERRY', 'BANANA', 'BERRY', 'BANANA', 'BLUEBERRY', 'BLACKBERRY', 'BANANA', 'BERRY', 'BANANA', 'BLUEBERRY', 'BLACKBERRY', 'BANANA', 'BERRY', 'BANANA', 'BLUEBERRY', 'BLACKBERRY', 'BANANA', 'BERRY', 'BANANA', 'BLUEBERRY', 'BLACKBERRY', 'BANANA', 'BERRY', 'BANANA', 'BLUEBERRY', 'BLACKBERRY', 'BANANA', 'BERRY', 'BANANA', 'BLUEBERRY', 'BLACKBERRY', 'BANANA', 'BERRY', 'BANANA', 'BLUEBERRY', 'BLACKBERRY', 'BANANA', 'BERRY', 'BANANA', 'BLUEBERRY', 'BLACKBERRY', 'BANANA', 'BERRY', 'BANANA', 'BLUEBERRY', 'BLACKBERRY', 'BANANA', 'BERRY', 'BANANA', 'BLUEBERRY', 'BLACKBERRY', 'BANANA', 'BERRY', 'BANANA', 'BLUEBERRY', 'BLACKBERRY', 'BANANA', 'BERRY']\n",
            "üîπ –£–Ω—ñ–∫–∞–ª—å–Ω—ñ —Å–ª–æ–≤–∞: ['BERRY', 'BANANA', 'BLUEBERRY', 'BLACKBERRY']\n",
            "\n",
            "üîπ –ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–∞—Ä–Ω–∏—Ö —á–∏—Å–µ–ª: 48\n",
            "üîπ –ü–µ—Ä—à—ñ 10 –∫–≤–∞–¥—Ä–∞—Ç—ñ–≤: [2601, 6561, 3969, 8836, 10000, 2025, 9801, 169, 9604, 64]\n",
            "\n",
            "üîπ –°—É–º–∞ –≤—Å—ñ—Ö —á–∏—Å–µ–ª: 4880\n",
            "üîπ –î–æ–±—É—Ç–æ–∫ —É—Å—ñ—Ö —á–∏—Å–µ–ª: 4317914945725370031110899965587667797431014121530232076524012469612701101301995952320379507742831936506573197581467888226287859898777600000000000000000000000\n",
            "\n",
            "–ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ (1-–π —Ä–∞–∑): 100\n",
            "–ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ (2-–π —Ä–∞–∑ —ñ–∑ –∫–µ—à—É): 100\n",
            "\n",
            "üîπ –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ª—ñ–≤ —É –¥–∞—Ç–∞—Å–µ—Ç—ñ:\n",
            "this: 2\n",
            "an: 1\n",
            "of: 1\n",
            "word: 1\n",
            "count.: 1\n",
            "rdd: 1\n",
            "operations: 1\n",
            "are: 1\n",
            "powerful.: 1\n",
            "map: 1\n",
            "and: 1\n",
            "reducebykey: 1\n",
            "is: 1\n",
            "example: 2\n",
            "pyspark: 2\n",
            "very: 1\n",
            "demonstrates: 1\n",
            "in: 1\n",
            "spark.: 1\n",
            "\n",
            "üîπ –¢–æ–ø 10 IP —ñ–∑ —É—Å–ø—ñ—à–Ω–∏–º–∏ –∑–∞–ø–∏—Ç–∞–º–∏:\n",
            "[('127.0.0.1', 60)]\n",
            "\n",
            "üîπ –ì—Ä—É–ø—É–≤–∞–Ω–Ω—è –∑–∞ —Å—Ç–∞—Ç—É—Å–∞–º–∏:\n",
            "200 ‚Üí 60\n",
            "404 ‚Üí 60\n",
            "\n",
            "üîπ 20 –Ω–∞–π—á–∞—Å—Ç—ñ—à–∏—Ö —Å–ª—ñ–≤ —É —Å—Ç–∞—Ç—Ç—ñ:\n",
            "spark: 1500\n",
            "such: 600\n",
            "pyspark: 600\n",
            "interface: 300\n",
            "apache: 300\n",
            "write: 300\n",
            "applications: 300\n",
            "using: 300\n",
            "python: 300\n",
            "supports: 300\n",
            "most: 300\n",
            "cover: 300\n",
            "workloads: 300\n",
            "batch: 300\n",
            "iterative: 300\n",
            "interactive: 300\n",
            "queries: 300\n",
            "allows: 300\n",
            "features: 300\n",
            "mllib: 300\n",
            "\n",
            "üîπ –ù–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–µ —Å–ª–æ–≤–æ: spark\n",
            "\n",
            "üîπ –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –ø—Ä–æ–¥–∞–∂—ñ–≤ –∑–∞ —Ç–æ–≤–∞—Ä–∞–º–∏:\n",
            "–ú–æ–Ω—ñ—Ç–æ—Ä: 20\n",
            "–ù–æ—É—Ç–±—É–∫: 30\n",
            "–°–º–∞—Ä—Ç—Ñ–æ–Ω: 30\n",
            "–ü–ª–∞–Ω—à–µ—Ç: 20\n",
            "\n",
            "üîπ –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∞–Ω–æ–º–∞–ª—å–Ω–æ –≤–∏—Å–æ–∫–∏—Ö —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä: 5\n",
            "\n",
            "‚úÖ –í–ò–°–ù–û–í–ö–ò:\n",
            "- –°—Ç–≤–æ—Ä–µ–Ω–æ —Ç–∞ –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–æ RDD-—Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –≤ PySpark.\n",
            "- –í–∏–∫–æ–Ω–∞–Ω–æ –±–∞–∑–æ–≤—ñ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó (map, filter, distinct, reduceByKey).\n",
            "- –ü—Ä–æ–≤–µ–¥–µ–Ω–æ –∞–≥—Ä–µ–≥–∞—Ü—ñ—é —Ç–∞ –∫–µ—à—É–≤–∞–Ω–Ω—è.\n",
            "- –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ broadcast- —ñ accumulator-–∑–º—ñ–Ω–Ω—ñ.\n",
            "- –ü—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–æ–≤–∞–Ω–æ —Ä–æ–±–æ—Ç—É –∑ —Ç–µ–∫—Å—Ç–∞–º–∏, –ª–æ–≥–∞–º–∏, —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—è–º–∏ —Ç–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞–º–∏.\n",
            "PySpark –¥–∞—î –∑–º–æ–≥—É –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–æ–±–ª—è—Ç–∏ –≤–µ–ª–∏–∫—ñ –æ–±—Å—è–≥–∏ –¥–∞–Ω–∏—Ö —É —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–æ–º—É —Å–µ—Ä–µ–¥–æ–≤–∏—â—ñ.\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPILc/50O7arIs2DWiKYPck",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}